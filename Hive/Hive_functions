Script started on Tue 17 Mar 2020 04:33:42 AM PDT
]0;cloudera@quickstart:~[?1034h[cloudera@quickstart ~]$ Hive
bash: Hive: command not found
]0;cloudera@quickstart:~[cloudera@quickstart ~]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> u[7G[Kselect unix_timestamp('203[32G[K20=[34G[K-01-31' 00:00:00');[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G 00:00:00');[K[40G[41G[42G[43G[44G[45G[46G[47G[48G[49G[50G[51G[52G
OK
1580457600
Time taken: 6.144 seconds, Fetched: 1 row(s)
hive> select from_unixtime(1580457600);
OK
2020-01-31 00:00:00
Time taken: 0.238 seconds, Fetched: 1 row(s)
hive> select(0[14G[K[13G[K ceil(9.24);
OK
10
Time taken: 0.259 seconds, Fetched: 1 row(s)
hive> select floor [19G[K(9.24);
OK
9
Time taken: 0.188 seconds, Fetched: 1 row(s)
hive> round[11G[K[10G[K[9G[K[8G[K[7G[Kselect round(123.45675656,2);
OK
123.46
Time taken: 0.239 seconds, Fetched: 1 row(s)
hive> select(23);
OK
23
Time taken: 0.149 seconds, Fetched: 1 row(s)
hive> sl[8G[Kelect(24);
OK
24
Time taken: 0.182 seconds, Fetched: 1 row(s)
hive> select * from information;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'information'
hive> show databases;
OK
d1
d2
d3
default
Time taken: 0.146 seconds, Fetched: 4 row(s)
hive> use d2;
OK
Time taken: 0.101 seconds
hive> 
    > 
    > seect[11G[K[10G[K[9G[Kec[10G[K[9G[Klect * from table2;
OK
499	["Poole","GBR"]	England	141000
501	["Blackburn","GBR"]	England	140000
500	["Bolton","GBR"]	England	139020
502	["Newport","GBR"]	Wales	139000
503	["PrestON","GBR"]	England	135000
504	["Stockport","GBR"]	England	132813
Time taken: 0.331 seconds, Fetched: 6 row(s)
hive> 
    > select concat(col1, '-', col3);[37G[K rom [41G[K[40G[K[39G[K[38G[Kfrom tableq;[49G[K[48G[K1;[49G[K[48G[K2;
OK
499-England
501-England
500-England
502-Wales
503-England
504-England
Time taken: 0.327 seconds, Fetched: 6 row(s)
hive> 
    > select length(cl[22G[Kol3) from table2;
OK
7
7
7
5
7
7
Time taken: 0.207 seconds, Fetched: 6 row(s)
hive> select lower(cole[23G[K3) from table2;
OK
england
england
england
wales
england
england
Time taken: 0.229 seconds, Fetched: 6 row(s)
hive> upper[11G[K[10G[K[9G[K[8G[K[7G[Kselectr [14G[K[13G[K upper(col3) from table2;
OK
ENGLAND
ENGLAND
ENGLAND
WALES
ENGLAND
ENGLAND
Time taken: 0.22 seconds, Fetched: 6 row(s)
hive> select lpad(col3,10,4[27G[K's[28G[Kv');
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'col3': (possible column names are: )
hive> select lpad(col3,10,'v');[31G[K from table2;
OK
vvvEngland
vvvEngland
vvvEngland
vvvvvWales
vvvEngland
vvvEngland
Time taken: 0.2 seconds, Fetched: 6 row(s)
hive> select lpad(col3,10,'v') from table2;[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14Gpad(col3,10,'v') from table2;[K[14Grpad(col3,10,'v') from table2;[15G[44G
OK
Englandvvv
Englandvvv
Englandvvv
Walesvvvvv
Englandvvv
Englandvvv
Time taken: 0.164 seconds, Fetched: 6 row(s)
hive> select t[14G[Kltrimp[19G[K)[19G[K[18G[Km('     ed[27G[K[26G[Kvedant');[34G[K;
OK
vedant
Time taken: 0.336 seconds, Fetched: 1 row(s)
hive> select[12G[K[11G[K[10G[K[9G[K[8G[K[7G[K
    > 
    > select rtrim('vedant/[27G[K   ');
OK
vedant
Time taken: 0.21 seconds, Fetched: 1 row(s)
hive> select repeat(col3,2) frr[31G[Kom table2;
OK
EnglandEngland
EnglandEngland
EnglandEngland
WalesWales
EnglandEngland
EnglandEngland
Time taken: 0.184 seconds, Fetched: 6 row(s)
hive> s[7G[K
    > 
    > select(reve[17G[K[16G[K[15G[K[14G[K[13G[K reverse(c[22G[Kcol3) from table2;
OK
dnalgnE
dnalgnE
dnalgnE
selaW
dnalgnE
dnalgnE
Time taken: 0.235 seconds, Fetched: 6 row(s)
hive> '[7G[Kselect sp[15G=p[16G[15Gp[K[15G[14Gp[K[14G[13Gp[K[13G p[14Gsp[15Gpp[16Glp[17Gip[18Gtp[19G[20G[19G[K(hive:hadp[28G[Koop[30G[K[29G[K[28G[K[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K'hive:hadoop;[32G[K')[33G[K,':');
OK
["hive","hadoop"]
Time taken: 0.19 seconds, Fetched: 1 row(s)
hive> 
    > select substr('hive i [28G[Ks quering t[38G[K[37G[K[36G[K[35G[K[34G[K[33G[Krying tool','[45G[K4zf[47G[K[46G[K);
OK
e is querying tool
Time taken: 0.18 seconds, Fetched: 1 row(s)
hive> select substr('hive is quering tool'[42G[K[41G[K[40G[K[39G[K[38G[K[37G[K[36G[K[35G[K[34G[Kyiing [39G[K[38G[K[37G[K[36G[Kng tool',3[45G[K4,70[48G[K);
OK
e is qu
Time taken: 0.18 seconds, Fetched: 1 row(s)
hive> select int[16G[Kstr('hive is quir[32G[K[31G[Keri[33G[Kying tol[40G[Kol','e');
OK
4
Time taken: 0.165 seconds, Fetched: 1 row(s)
hive> select instr('hive is querying tool','e');[48G[47G[46G ');[47G,');[48G[47G');[K[47G[46G');[K[46G[47G,);[48G );[49G');[50Ge);[51G');[52G[54G
FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch ''e'': The function INSTR accepts exactly 2 arguments.
hive> 
    > select instr('hive is querying tool','e', 'e');[47G[K);
OK
4
Time taken: 0.168 seconds, Fetched: 1 row(s)
hive> 
    > select * from tabe[24G[Kfe[25G[K[24G[Kle2;
OK
499	["Poole","GBR"]	England	141000
501	["Blackburn","GBR"]	England	140000
500	["Bolton","GBR"]	England	139020
502	["Newport","GBR"]	Wales	139000
503	["PrestON","GBR"]	England	135000
504	["Stockport","GBR"]	England	132813
Time taken: 0.208 seconds, Fetched: 6 row(s)
hive> slecr[11G[Kt[11G[K[10G[K[9G[K[8G[K[7G[Kselect ;[14G[K[13G[K if [16G[K(col3='England"[30G[K',col2,col1);[42G[K form ta[49G[K[48G[K[47G[K[46G[K[45G[K[44G[Krom table2;
FAILED: SemanticException [Error 10016]: Line 1:30 Argument type mismatch 'col1': The second and the third arguments of function IF should have the same type, but they are different: "array<string>" and "string"
hive> select if(col3='England',col2,col1) from table2;[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[36G[35G,col1) from table2;[K[35G4,col1) from table2;[36G[55G
OK
141000
140000
139020
502
135000
132813
Time taken: 0.785 seconds, Fetched: 6 row(s)
hive> sle[9G[K[8G[Kelect sp[15G[K[14G[Kconcat(col1[24G[K2[24G[K3, slec[30G[K[29G[K[28G[Kelect[32G[K[31G[K[30G[K[29G[K[28G[K[27G[K'-', select co[40G[K[39G[Kif(col3='Engls[52G[Kand',col1,col4) from talb[76G[K[75G[Kbl3[77G[Ke2;[79G[K) [80G[K)[80G[K[79G[K);[80G[Kfrom table2);[92G[K[91G[K;
NoViableAltException(221@[147:1: selectExpression : ( expression | tableAllColumns );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpression(HiveParser_SelectClauseParser.java:4206)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectExpression(HiveParser.java:44332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:4562)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6538)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6641)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:7026)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:7086)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7270)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7430)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7590)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7750)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7909)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8439)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9452)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9571)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9730)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6363)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:44344)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectItem(HiveParser_SelectClauseParser.java:3058)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectList(HiveParser_SelectClauseParser.java:1320)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:1083)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:44365)
	at org.apache.hadoop.hive.ql.parse.HiveParser.singleSelectStatement(HiveParser.java:41487)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41193)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41130)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40183)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40059)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1519)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:25 cannot recognize input near 'select' 'if' '(' in select expression
hive> select concat(col3, '-', select if(col3='England',col1,col4) from table2)from table2;
NoViableAltException(221@[147:1: selectExpression : ( expression | tableAllColumns );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpression(HiveParser_SelectClauseParser.java:4206)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectExpression(HiveParser.java:44332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:4562)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6538)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6641)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:7026)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:7086)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7270)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7430)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7590)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7750)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7909)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8439)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9452)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9571)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9730)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6363)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:44344)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectItem(HiveParser_SelectClauseParser.java:3058)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectList(HiveParser_SelectClauseParser.java:1320)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:1083)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:44365)
	at org.apache.hadoop.hive.ql.parse.HiveParser.singleSelectStatement(HiveParser.java:41487)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41193)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41130)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40183)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40059)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1519)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:25 cannot recognize input near 'select' 'if' '(' in select expression
hive> 
    > 
    > 
    > 
    > sek[9G[K[8G[K[7G[Kselect concat(col3, '-', select if(col3='England',col1,col4) from table2)from table2;[14G[Kif(col3='England',col4,col1) from table2;[14G[Kconcat(col3, '-', select if(col3='England',col1,col4) from table2)from table2;[14G[Kif(col3='England',col4,col1) from table2;[35G[K2,col1) from table2;[35G[K4,col1) from table2;[14G[Kconcat(col3, '-', select if(col3='England',col1,col4) from table2)from table2;[7G[Kselect concat(col3, '-', select if(col3='England',col1,col4) from table2)from table2;[14G[Kif(col3='England',col4,col1) from table2;[35G[K2,col1) from table2;[14G[K* from table2;[14G[Kinstr('hive is querying tool','e');[47G[K, 'e');[47G[K);[14G[K* from table2;
OK
499	["Poole","GBR"]	England	141000
501	["Blackburn","GBR"]	England	140000
500	["Bolton","GBR"]	England	139020
502	["Newport","GBR"]	Wales	139000
503	["PrestON","GBR"]	England	135000
504	["Stockport","GBR"]	England	132813
Time taken: 0.173 seconds, Fetched: 6 row(s)
hive> selee[11G[Kct exploe[19G[Kdes[21G[K(col2) as myCol from table2;
OK
Poole
GBR
Blackburn
GBR
Bolton
GBR
Newport
GBR
PrestON
GBR
Stockport
GBR
Time taken: 0.38 seconds, Fetched: 12 row(s)
hive> select explode(col2) as myCol from table2;
OK
Poole
GBR
Blackburn
GBR
Bolton
GBR
Newport
GBR
PrestON
GBR
Stockport
GBR
Time taken: 0.205 seconds, Fetched: 12 row(s)
hive> select col3[17G[K3,col2 from table2;[35G[K lateral view exploe[54G[Kde(colw[60G[K2) dummy as n[72G[Koutput_col;
OK
England	["Poole","GBR"]
England	["Poole","GBR"]
England	["Blackburn","GBR"]
England	["Blackburn","GBR"]
England	["Bolton","GBR"]
England	["Bolton","GBR"]
Wales	["Newport","GBR"]
Wales	["Newport","GBR"]
England	["PrestON","GBR"]
England	["PrestON","GBR"]
England	["Stockport","GBR"]
England	["Stockport","GBR"]
Time taken: 0.247 seconds, Fetched: 12 row(s)
hive> select col3,col2 from table2 lateral view explode(col2) dummy as output_col;[7G[Kselect col3,col2 from table2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table2;[14G[K* from table2;[14G[Kexplode(col2) as myCol from table2;[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14G[49G
OK
Poole
GBR
Blackburn
GBR
Bolton
GBR
Newport
GBR
PrestON
GBR
Stockport
GBR
Time taken: 0.221 seconds, Fetched: 12 row(s)
hive> select explode(col2) as myCol from table2;[14G[Kcol3,col2 from table2 lateral view explode(col2) dummy as output_col;
OK
England	["Poole","GBR"]
England	["Poole","GBR"]
England	["Blackburn","GBR"]
England	["Blackburn","GBR"]
England	["Bolton","GBR"]
England	["Bolton","GBR"]
Wales	["Newport","GBR"]
Wales	["Newport","GBR"]
England	["PrestON","GBR"]
England	["PrestON","GBR"]
England	["Stockport","GBR"]
England	["Stockport","GBR"]
Time taken: 0.212 seconds, Fetched: 12 row(s)
hive> select col3,col2 from table2 lateral view explode(col2) dummy as output_col;[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[21G[22G[23G[22G from table2 lateral view explode(col2) dummy as output_col;[K[22G3 from table2 lateral view explode(col2) dummy as output_col;[23G[22G[21G[20G[19G[18G[17G,col3 from table2 lateral view explode(col2) dummy as output_col;[K[17G2,col3 from table2 lateral view explode(col2) dummy as output_col;[18G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[52G[53G[54G[55G[56G[57G[58G[59G[60G[61G[60G) dummy as output_col;[K[60G3) dummy as output_col;[61G[83G
FAILED: UDFArgumentException explode() takes an array or a map as a parameter
hive> select col2,col3 from table2 lateral view explode(col3) dummy as output_col;[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[61G[60G) dummy as output_col;[K[60G2) dummy as output_col;[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G,col3 from table2 lateral view explode(col2) dummy as output_col;[K[17G3,col3 from table2 lateral view explode(col2) dummy as output_col;[18G[19G[20G[21G[22G[23G[22G from table2 lateral view explode(col2) dummy as output_col;[K[22G2 from table2 lateral view explode(col2) dummy as output_col;[23G[22G from table2 lateral view explode(col2) dummy as output_col;[K[22G[21G from table2 lateral view explode(col2) dummy as output_col;[K[21G[20G from table2 lateral view explode(col2) dummy as output_col;[K[20G[19G from table2 lateral view explode(col2) dummy as output_col;[K[19G[18G from table2 lateral view explode(col2) dummy as output_col;[K[18G[17G from table2 lateral view explode(col2) dummy as output_col;[K[17G2 from table2 lateral view explode(col2) dummy as output_col;[18G, from table2 lateral view explode(col2) dummy as output_col;[19Gc from table2 lateral view explode(col2) dummy as output_col;[20Go from table2 lateral view explode(col2) dummy as output_col;[21G3 from table2 lateral view explode(col2) dummy as output_col;[22G[21G from table2 lateral view explode(col2) dummy as output_col;[K[21Gl from table2 lateral view explode(col2) dummy as output_col;[22Gl from table2 lateral view explode(col2) dummy as output_col;[23G[22G from table2 lateral view explode(col2) dummy as output_col;[K[22G3 from table2 lateral view explode(col2) dummy as output_col;[23G[83G
OK
["Poole","GBR"]	England
["Poole","GBR"]	England
["Blackburn","GBR"]	England
["Blackburn","GBR"]	England
["Bolton","GBR"]	England
["Bolton","GBR"]	England
["Newport","GBR"]	Wales
["Newport","GBR"]	Wales
["PrestON","GBR"]	England
["PrestON","GBR"]	England
["Stockport","GBR"]	England
["Stockport","GBR"]	England
Time taken: 0.237 seconds, Fetched: 12 row(s)
hive> select col2,col3 from table2 lateral view explode(col2) dummy as output_col;[60G[K3) dummy as output_col;
FAILED: UDFArgumentException explode() takes an array or a map as a parameter
hive> select col2,col3 from table2 lateral view explode(col3) dummy as output_col;[60G[K2) dummy as output_col;[60G[K3) dummy as output_col;[17G[K3,col2 from table2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table2;[14G[Kcol3,col2 from table2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table2;[14G[Kcol3,col2 from table2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table2;[14G[Kcol3,col2 from table2 lateral view explode(col2) dummy as output_col;
OK
England	["Poole","GBR"]
England	["Poole","GBR"]
England	["Blackburn","GBR"]
England	["Blackburn","GBR"]
England	["Bolton","GBR"]
England	["Bolton","GBR"]
Wales	["Newport","GBR"]
Wales	["Newport","GBR"]
England	["PrestON","GBR"]
England	["PrestON","GBR"]
England	["Stockport","GBR"]
England	["Stockport","GBR"]
Time taken: 0.172 seconds, Fetched: 12 row(s)
hive>     [7Gs    [8Ge    [9Gl    [10Ge    [11Gc    [12Gt    [13G     [14Gc    [15G[14G    [K[14Ge    [15Gx    [16Gp    [17G[16G    [K[16Gp    [17Gl    [18Go    [19Ge    [20G[19G    [K[19Gd    [20Ge    [21G(    [22Gc    [23Go    [24Gl    [25G3    [26G[25G    [K[25G2    [26GO    [27G[26G    [K[26G(    [27G[26G    [K[26G)    [27G     [28Gf    [29Gr    [30Go    [31Gm    [32G     [33Gt    [34Ga    [35Gb    [36Ge    [37G[36G    [K[36Ge    [37G[36G    [K[36Gl    [37Ge    [38G2    [39G;    [40G[41G
OK
Poole
GBR
Blackburn
GBR
Bolton
GBR
Newport
GBR
PrestON
GBR
Stockport
GBR
Time taken: 0.172 seconds, Fetched: 12 row(s)
hive> select col3,co2,[22G[K from c[28G[Ktable2;[34G[K ;[35G[Kl[35G[K[34G[K;
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'co2': (possible column names are: col1, col2, col3, col4)
hive> select col3,co2 from table2;[34G[K lateral view explode(col2) du[63G[K[62G[K[61G[K;
NoViableAltException(-1@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:11322)
	at org.apache.hadoop.hive.ql.parse.HiveParser.identifier(HiveParser.java:44335)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableAlias(HiveParser_FromClauseParser.java:3535)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.lateralView(HiveParser_FromClauseParser.java:3355)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3708)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1823)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1478)
	at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:44339)
	at org.apache.hadoop.hive.ql.parse.HiveParser.singleSelectStatement(HiveParser.java:41508)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41193)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41130)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40183)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40059)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1519)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:54 cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in table alias
hive> select col3,co2 from table2 lateral view explode(col2);[61G[Kducc[64G[K[63G[Kmj[64G[Kmu[65G[Ky s[67G[Kas oup[72G[Ktput_cole[80G[K;
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'co2': (possible column names are: table2.col1, table2.col2, table2.col3, table2.col4, dummy.output_col)
hive> select col3,co2 from table2 lateral view explode(col2)dummy as output_col;[61G[K;[34G[K;[14G[Kexplode(col2) from table2;    [14G[Kcol3,col2 from table2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) from table2;    [14G[Kcol3,co2 from table2;[34G[K lateral view explode(col2);[61G[Kdummy as output_col;[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[7G)dummy as output_col;[K[7G[Kselect col3,co2 from table2 lateral view explode(col2)dummy as output_col;[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G dummy as output_col;[62G[82G
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'co2': (possible column names are: table2.col1, table2.col2, table2.col3, table2.col4, dummy.output_col)
hive> select col3,co2 from table2 lateral view explode(col2) dummy as output_col;[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[20G[21Gl2 from table2 lateral view explode(col2) dummy as output_col;[22G[83G
OK
England	["Poole","GBR"]
England	["Poole","GBR"]
England	["Blackburn","GBR"]
England	["Blackburn","GBR"]
England	["Bolton","GBR"]
England	["Bolton","GBR"]
Wales	["Newport","GBR"]
Wales	["Newport","GBR"]
England	["PrestON","GBR"]
England	["PrestON","GBR"]
England	["Stockport","GBR"]
England	["Stockport","GBR"]
Time taken: 0.206 seconds, Fetched: 12 row(s)
hive> create tabe[17G[Kle [19G[K[18G[K[17G[K[16G[K[15G[K[14G[Kexternal table if not exit [40G[Ks tabl3[46G[Ke10(col1 string, co2 [66G[K[65G[Kl2 array<string>) row format delimited fir[106G[Keld by','[114G[113G[112G[111G[110G[109Gs by','[110G[111G[112G[113G[114G[115G[116G,[116G[Kco lection item terminated by':'lines terminated by '][51G[K\m\[53G[K[52G[Kn' sorted as te[66G[Kwx[67G[K[66G[Kxt[67G[K[66G[Kextf[69G[Kfile;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4686)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:29 missing KW_EXISTS at 'exits' near 'exits' in create table statement
line 1:35 cannot recognize input near 'table10' '(' 'col1' in create table statement
hive> create external table if not exits table10(col1 string, col2 array<string>) row format delimited fields by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[7G[K[K[B[2K[Aselect col3,col2 from table2 lateral view explode(col2) dummy as output_col;[7G[Kcreate external table if not exits table10(col1 string, col2 array<string>) row format delimited fields by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[1A[7G[Kated by '\n' sorted as textfile;[K[B[2K[A[7G[Kselect col3,col2 from table2 lateral view explode(col2) dummy as output_col;[7G[Kcreate external table if not exits table10(col1 string, col2 array<string>) row format delimited fields by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14G[13G[12G[11G[10G[9G[8G[7G[6G[5G[4G[3G[2G[1G[1A[117G[116G[115G[114G[113G[112G[111G[110G[109G[108G[107G[106G[105G[104G[103G[102G[101G[100G[99G[98G[97G[96G[95G[94G[93G[92G[91G[90G[89G[88G[87G[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[41G[40G table10(col1 string, col2 array<string>) row format delimited fields by','col ection item terminated by':'lines terminated by '\n' sorted as textfile;[K[1A[40G[39G table10(col1 string, col2 array<string>) row format delimited fields by','cole ction item terminated by':'lines terminated by '\n' sorted as textfile;[K[1A[39G[38G table10(col1 string, col2 array<string>) row format delimited fields by','colec tion item terminated by':'lines terminated by '\n' sorted as textfile;[K[1A[38Gs table10(col1 string, col2 array<string>) row format delimited fields by','cole ction item terminated by':'lines terminated by '\n' sorted as textfile;[1A[39G[38G table10(col1 string, col2 array<string>) row format delimited fields by','colec tion item terminated by':'lines terminated by '\n' sorted as textfile;[K[1A[38Gi table10(col1 string, col2 array<string>) row format delimited fields by','cole ction item terminated by':'lines terminated by '\n' sorted as textfile;[1A[39Gs table10(col1 string, col2 array<string>) row format delimited fields by','col ection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[40Gt table10(col1 string, col2 array<string>) row format delimited fields by','co lection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[41Gs table10(col1 string, col2 array<string>) row format delimited fields by','c olection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[42G[1B[75G
NoViableAltException(26@[1750:103: ( tableRowFormatMapKeysIdentifier )?])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.rowFormatDelimited(HiveParser.java:33960)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableRowFormat(HiveParser.java:34195)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4979)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:105 missing TERMINATED at 'by' near '','' in serde properties specification
line 1:110 cannot recognize input near 'colection' 'item' 'terminated' in serde properties specification
hive> create external table if not exists table10(col1 string, col2 array<string>) row format delimited fields by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14G[13G[12G[11G[10G[9G[8G[7G[6G[5G[4G[3G[2G[1G[1A[117G[116G[115G[114G[113G[112G[111G[110G[109G[108G[107G[106G[105G[104G[105G[106G[107G[108G[109G[110G[111G[112G[111G  by',' colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[112Gt by', 'colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[113Ge by' ,'colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[114Gr by ','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[115Gm b y','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[116Gi  by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[117Gn by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1Gi by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[2G[1G by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[K[1Ga by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[2Gt by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[3Ge by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[4Gd by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[5G  by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[6G[5G by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[K[5G[86G
NoViableAltException(26@[1750:103: ( tableRowFormatMapKeysIdentifier )?])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.rowFormatDelimited(HiveParser.java:33960)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableRowFormat(HiveParser.java:34195)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4979)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:121 cannot recognize input near 'colection' 'item' 'terminated' in serde properties specification
hive> create external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[24G[25G[26G[25Gterminated by':'lines terminated by '\n' sorted as textfile;[K[25Gsterminated by':'lines terminated by '\n' sorted as textfile;[26G terminated by':'lines terminated by '\n' sorted as textfile;[27G[87G
NoViableAltException(26@[1750:103: ( tableRowFormatMapKeysIdentifier )?])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.rowFormatDelimited(HiveParser.java:33960)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableRowFormat(HiveParser.java:34195)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4979)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:121 cannot recognize input near 'colection' 'items' 'terminated' in serde properties specification
hive> create external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','colection items terminated by':'lines terminated by '\n' sorted as textfile;[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14G[87G
NoViableAltException(26@[1750:103: ( tableRowFormatMapKeysIdentifier )?])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.rowFormatDelimited(HiveParser.java:33960)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableRowFormat(HiveParser.java:34195)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4979)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:121 cannot recognize input near 'colection' 'items' 'terminated' in serde properties specification
hive> create external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','colection items terminated by':'lines terminated by '\n' sorted as textfile;[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14Glection items terminated by':'lines terminated by '\n' sorted as textfile;[15G[88G
FAILED: ParseException line 1:179 missing EOF at 'sorted' near ''\n''
hive> create external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','collection items terminated by':'lines terminated by '\n' sorted as textfile;[87G[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[39G[40G[41G[42G[43G[44G[45G[46G[47G[48G[49G[50G[51G[52G[53G[54G[55G[56G[57G[58G[59G[60G[61G[62G[63G[64G[65G[66G[67G[68G[69G[70G[71G[72G[73G[74G[75G[74G as textfile;[K[74G[73G as textfile;[K[73G[72G as textfile;[K[72G[71G as textfile;[K[71G[70G as textfile;[K[70Gt as textfile;[71Go as textfile;[72Gr as textfile;[73Ge as textfile;[74Gd as textfile;[75G[88G
OK
Time taken: 0.307 seconds
hive> load datalocal inpath '/home/cloudera/Downloads/expode.txt' into table tablee[83G[K10;[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58Glode.txt' into table table10;[59G[87G
MismatchedTokenException(26!=71)
	at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617)
	at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)
	at org.apache.hadoop.hive.ql.parse.HiveParser.loadStatement(HiveParser.java:1693)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1534)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:5 mismatched input 'datalocal' expecting DATA near 'load' in load statement
hive> load datalocal inpath '/home/cloudera/Downloads/explode.txt' into table table10;[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G local inpath '/home/cloudera/Downloads/explode.txt' into table table10;[17G[88G
Loading data to table d2.table10
Table d2.table10 stats: [numFiles=1, totalSize=123]
OK
Time taken: 0.978 seconds
hive> seec[10G[K[9G[Klect * form [20G[K[19G[K[18G[K[17G[Krom table 1-=[29G[K[28G[K0;[29G[K[28G[K[27G[K[26G[K[25G[Ke10;
OK
Salman Rushide	["Grimus","Shame","Fury"]
Thomas Otway	["Don Carlos","The Orphan"]
Ben Jonson	["Volpone","Epicene"]
John Milton	["Arcades","Comus"]
Time taken: 0.152 seconds, Fetched: 4 row(s)
hive> set hive.cli.print.heade[30G[Ker = true;
hive> set hive.cli.print.header = true;[9G[Klect * from table10;
OK
table10.col1	table10.col2
Salman Rushide	["Grimus","Shame","Fury"]
Thomas Otway	["Don Carlos","The Orphan"]
Ben Jonson	["Volpone","Epicene"]
John Milton	["Arcades","Comus"]
Time taken: 0.147 seconds, Fetched: 4 row(s)
hive> select c[14G[Kexploed[20G[K[19G[Kde([21G[Ks(so[24G[K[23G[Kcol2) as mycol[36G[K[35G[K[34G[KCol from table10;
FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function 'explodes'
hive> select explodes(col2) as myCol from table10;[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[22G[21G(col2) as myCol from table10;[K[21G[50G
OK
mycol
Grimus
Shame
Fury
Don Carlos
The Orphan
Volpone
Epicene
Arcades
Comus
Time taken: 0.153 seconds, Fetched: 9 row(s)
hive> select explode(col2) as myCol from table10;[21G[Ks(col2) as myCol from table10;[14G[K* from table10;[9G[Kt hive.cli.print.header = true;[9G[Klect * from table10;[7G[Kload data local inpath '/home/cloudera/Downloads/explode.txt' into table table10;[16G[Klocal inpath '/home/cloudera/Downloads/explode.txt' into table table10;[7G[Kcreate external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','collection items terminated by':'lines terminated by '\n' stored as textfile;[70G[Korted as textfile;[14G[Kection items terminated by':'lines terminated by '\n' sorted as textfile;[25G[K terminated by':'lines terminated by '\n' sorted as textfile;[1A[112G[K[K[B[2K[Aby','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[39G[K[K[B[2K[Ats table10(col1 string, col2 array<string>) row format delimited fields by','colection item terminated by':'lines terminated by '\n' sorted as textfile;[1A[7G[K[K[B[2K[Aselect col3,col2 from table2 lateral view explode(col2) dummy as output_col;[21G[K2 from table2 lateral view explode(col2) dummy as output_col;[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[17G[18G[17G,co2 from table2 lateral view explode(col2) dummy as output_col;[K[17G1,co2 from table2 lateral view explode(col2) dummy as output_col;[18G[19G[20G[21G[22G[23G[24G[25G[26G[27G[28G[29G[30G[31G[32G[33G[34G[35G[36G[37G[38G[39G[40G[41G[42G[43G[44G[45G[46G[47G[48G[49G[50G[51G[52G[53G[54G[55G[82G
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'co2': (possible column names are: table2.col1, table2.col2, table2.col3, table2.col4, dummy.output_col)
hive> select col1,co2 from table2 lateral view explode(col2) dummy as output_col;[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[31G[32G[33G[34G[33G lateral view explode(col2) dummy as output_col;[K[33G1 lateral view explode(col2) dummy as output_col;[34G0 lateral view explode(col2) dummy as output_col;[35G[83G
FAILED: SemanticException [Error 10004]: Line 1:12 Invalid table alias or column reference 'co2': (possible column names are: table10.col1, table10.col2, dummy.output_col)
hive> select col1,co2 from table10 lateral view explode(col2) dummy as output_col;[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21Gl2 from table10 lateral view explode(col2) dummy as output_col;[22G[84G
OK
col1	col2
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Thomas Otway	["Don Carlos","The Orphan"]
Thomas Otway	["Don Carlos","The Orphan"]
Ben Jonson	["Volpone","Epicene"]
Ben Jonson	["Volpone","Epicene"]
John Milton	["Arcades","Comus"]
John Milton	["Arcades","Comus"]
Time taken: 0.229 seconds, Fetched: 9 row(s)
hive> select col1,col2 from table10 lateral view explode(col2) dummy as output_col;[83G[82G[81G[80G[79G[78G_col;[K[78G[77G_col;[K[77G[76G_col;[K[76G[75G_col;[K[75G[74G_col;[K[74G[73G_col;[K[73Gd_col;[74Gu_col;[75Gm_col;[76Gm_col;[77Gy_col;[78G[83G
OK
col1	col2
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Thomas Otway	["Don Carlos","The Orphan"]
Thomas Otway	["Don Carlos","The Orphan"]
Ben Jonson	["Volpone","Epicene"]
Ben Jonson	["Volpone","Epicene"]
John Milton	["Arcades","Comus"]
John Milton	["Arcades","Comus"]
Time taken: 0.177 seconds, Fetched: 9 row(s)
hive> select 'hadoop' rlike 'ha';
OK
_c0
true
Time taken: 0.171 seconds, Fetched: 1 row(s)
hive> select 'hadoop' rlike 'ha';[14G[Kcol1,col2 from table10 lateral view explode(col2) dummy as dummy_col;[82G,;[83Ga;[84Gn;[85Gt;[86G[85G;[K[85Go;[86Gt;[87Gh;[88Ge;[89Gr;[90G ;[91Gc;[92Go;[93Gl;[94G,;[95G ;[96Gw;[97Ga;[98Gt;[99G[98G;[K[98G[97G;[K[97G[96G;[K[96Go;[97Gh;[98G,;[99G[98G;[K[98G_;[99Gc;[100Go;[101Gl;[102G[101G[100G[99G[98G[97G[96G[95G[94G[93G[92G[91G[90Gcol, oh_col;[K[90G_col, oh_col;[91G[103G
FAILED: SemanticException [Error 10083]: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 1 aliases but got 3
hive> select col1,col2 from table10 lateral view explode(col2) dummy as dummy_col,another_col, oh_col;[14G[K'hadoop' rlike 'ha';[14G[Kcol1,col2 from table10 lateral view explode(col2) dummy as dummy_col;[73G[Koutput_col;
OK
col1	col2
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Salman Rushide	["Grimus","Shame","Fury"]
Thomas Otway	["Don Carlos","The Orphan"]
Thomas Otway	["Don Carlos","The Orphan"]
Ben Jonson	["Volpone","Epicene"]
Ben Jonson	["Volpone","Epicene"]
John Milton	["Arcades","Comus"]
John Milton	["Arcades","Comus"]
Time taken: 0.167 seconds, Fetched: 9 row(s)
hive> select col1,col2 from table10 lateral view explode(col2) dummy as output_col;[73G[Kdummy_col,another_col, oh_col;[14G[K'hadoop' rlike 'ha';[14G[Kcol1,col2 from table10 lateral view explode(col2) dummy as dummy_col;[73G[Koutput_col;[21G[K2 from table10 lateral view explode(col2) dummy as output_col;[33G[K2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table10;[21G[Ks(col2) as myCol from table10;[14G[K* from table10;[9G[Kt hive.cli.print.header = true;[9G[Klect * from table10;[7G[Kload data local inpath '/home/cloudera/Downloads/explode.txt' into table table10;[16G[Klocal inpath '/home/cloudera/Downloads/explode.txt' into table table10;[7G[Kcreate external table if not exists table10(col1 string, col2 array<string>) row format delimited fields terminated by','collection items terminated by':'lines terminated by '\n' stored as textfile;[87G[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[22G[21G[20G[19G[18G[17G[16G[15G[14G[13G[12G[11G[10G[9G[8G[7G[6G[5G[4G[3G[2G[1G[1A[117G[116G[115G[114G[113G[112G[111G[110G[109G[108G[107G[106G[105G[104G[103G[102G[101G[100G[99G[98G[97G[96G[95G[94G[93G[92G[91G[90G[89G[88G[87G[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[50G[49G(col1 string, col2 array<string>) row format delimited fields termina ted by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[49G[48G(col1 string, col2 array<string>) row format delimited fields terminat ed by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[48G[47G(col1 string, col2 array<string>) row format delimited fields terminate d by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[47G[46G(col1 string, col2 array<string>) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[46G[45G(col1 string, col2 array<string>) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[45G[44G(col1 string, col2 array<string>) row format delimited fields terminated b y','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[44G[43G(col1 string, col2 array<string>) row format delimited fields terminated by ','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[43Gr(col1 string, col2 array<string>) row format delimited fields terminated b y','collection items terminated by':'lines terminated by '\n' stored as textfile;[1A[44Ga(col1 string, col2 array<string>) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[1A[45Gn(col1 string, col2 array<string>) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[1A[46Gk(col1 string, col2 array<string>) row format delimited fields terminate d by','collection items terminated by':'lines terminated by '\n' stored as textfile;[1A[47G[48G[49G[50G[51G[52G[53G[54G[55G[56G[57G[58G[59G[60G[61G[62G[63G[64G[65G[66G[67G[68G[69G[70G[71G[72G[73G[74G[75G[76G[77G[78G[79G[78G) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[78G[77G) row format delimited fields terminated  by','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[77G[76G) row format delimited fields terminated b y','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[76G[75G) row format delimited fields terminated by ','collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[75G[74G) row format delimited fields terminated by' ,'collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[74G[73G) row format delimited fields terminated by', 'collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[73G[72G) row format delimited fields terminated by',' collection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[72G[71G) row format delimited fields terminated by','c ollection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[71G[70G) row format delimited fields terminated by','co llection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[70G[69G) row format delimited fields terminated by','col lection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[69G[68G) row format delimited fields terminated by','coll ection items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[68G[67G) row format delimited fields terminated by','colle ction items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[67G[66G) row format delimited fields terminated by','collec tion items terminated by':'lines terminated by '\n' stored as textfile;[K[1A[66Gi) row format delimited fields terminated by','colle ction items terminated by':'lines terminated by '\n' stored as textfile;[1A[67Gn) row format delimited fields terminated by','coll ection items terminated by':'lines terminated by '\n' stored as textfile;[1A[68Gt) row format delimited fields terminated by','col lection items terminated by':'lines terminated by '\n' stored as textfile;[1A[69G[70G[71G[72G[73G[74G[75G[76G[77G[78G[79G[80G[81G[82G[83G[84G[85G[86G[87G[88G[89G[90G[91G[92G[93G[94G[95G[96G[97G[98G[99G[100G[101G[102G[103G[104G[105G[106G[107G[108G[109G[110G[111G[112G[113G[114G[115G[116G[117G[1B[1G[2G[3G[4G[5G[6G[7G[8G[9G[10G[11G[12G[13G[14G[15G[16G[17G[18G[19G[20G[21G[22G[23G[24G[25G[26G[27G[28G[29G[30G[31G[32G[33G[34G[35G[36G[37G[38G[39G[40G[41G[42G[43G[44G[45G[46G[47G[48G[49G[50G[51G[52G[53G[54G[55G[56G[57G[58G[59G[60G[61G[62G[63G[64G[65G[66G[67G[68G[69G[70G[71G[72G[73G[74G[75G
OK
Time taken: 0.212 seconds
hive> create external table if not exists rank(col1 string, col2 int) row format delimited fields terminated by','collection items terminated by':'lines terminated by '\n' stored as textfile;[1A[7G[K[K[B[2K[Aselect col1,col2 from table10 lateral view explode(col2) dummy as output_col;[73G[Kdummy_col,another_col, oh_col;[14G[K'hadoop' rlike 'ha';[14G[Kcol1,col2 from table10 lateral view explode(col2) dummy as dummy_col;[73G[Koutput_col;[21G[K2 from table10 lateral view explode(col2) dummy as output_col;[33G[K2 lateral view explode(col2) dummy as output_col;[14G[Kexplode(col2) as myCol from table10;[21G[Ks(col2) as myCol from table10;[14G[K* from table10;[9G[Kt hive.cli.print.header = true;[9G[Klect * from table10;[7G[Kload data local inpath '/home/cloudera/Downloads/explode.txt' into table table10;[16G[Klocal inpath '/home/cloudera/Downloads/explode.txt' into table table10;[16G[K local inpath '/home/cloudera/Downloads/explode.txt' into table table10;[87G[86G[85G[84G[83G[82G[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G.txt' into table table10;[K[62G[61G.txt' into table table10;[K[61G[60G.txt' into table table10;[K[60G[59G.txt' into table table10;[K[59G[58G.txt' into table table10;[K[58G[57G.txt' into table table10;[K[57G[56G.txt' into table table10;[K[56Gr.txt' into table table10;[57Ga.txt' into table table10;[58Gn.txt' into table table10;[59Gk.txt' into table table10;[60G[61G[62G[63G[64G[65G[66G[67G[68G[69G[70G[71G[72G[73G[74G[75G[76G[77G[78G[79G[80G[81G[82G[83G[84G[85G[84G[83G;[K[83G[82G;[K[82G[81G;[K[81G[80G;[K[80G[79G;[K[79G[78G;[K[78G[77G;[K[77Gr;[78Ga;[79Gn;[80Gj;[81G[80G;[K[80Gk;[81G[82G
FAILED: SemanticException Line 1:23 Invalid path ''/home/cloudera/Downloads/rank.txt'': No files matching path file:/home/cloudera/Downloads/rank.txt
hive> load data local inpath '/home/cloudera/Downloads/rank.txt' into table rank;<<<<<<<<<<<<<<<<[97G[K[96G[K[95G[K[94G[K[93G[K[92G[K[91G[K[90G[K[89G[K[88G[K[87G[K[86G[K[85G[K[84G[K[83G[K[82G[K[81G[80G[79G[78G[77G[76G[75G[74G[73G[72G[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G.txt' into table rank;[K[59G[81G
Loading data to table d2.rank
Table d2.rank stats: [numFiles=1, totalSize=153]
OK
Time taken: 0.704 seconds
hive> select * from rank;
OK
rank.col1	rank.col2
John	1500
Albert	1500
Mark	1000
Frank	1150
Loopa	1100
Lui	1300
John	1300
John	900
Lesa	1500
Lesa	900
Pars	800
leo	700
leo	1500
lock	650
Bhut	800
Lio	500
Time taken: 0.131 seconds, Fetched: 16 row(s)
hive> 
    > select colw[17G[K2[17G[K1,col2 [23G[K,rank() over(c[36G[Kc[36G[Korder by col2 desc) from rank;
Query ID = cloudera_20200317085858_c236d931-a26b-47d7-b283-c75fbffc7cf0
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1584300088534_0008, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1584300088534_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1584300088534_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-03-17 08:59:29,838 Stage-1 map = 0%,  reduce = 0%
2020-03-17 09:00:01,241 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.73 sec
2020-03-17 09:00:27,273 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 3.7 sec
2020-03-17 09:00:30,887 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.87 sec
MapReduce Total cumulative CPU time: 4 seconds 870 msec
Ended Job = job_1584300088534_0008
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.87 sec   HDFS Read: 7787 HDFS Write: 192 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 870 msec
OK
col1	col2	_wcol0
leo	1500	1
Albert	1500	1
Lesa	1500	1
John	1500	1
John	1300	5
Lui	1300	5
Frank	1150	7
Loopa	1100	8
Mark	1000	9
John	900	10
Lesa	900	10
Pars	800	12
Bhut	800	12
leo	700	14
lock	650	15
Lio	500	16
Time taken: 106.008 seconds, Fetched: 16 row(s)
hive>  [7G[Kselect col1,col2,rank() over(order by col2 desc) from rank;[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G[32G[31G[30G[29G[28G[27G[26G[25G[24G[23G[24Gdrank() over(order by col2 desc) from rank;[25Gerank() over(order by col2 desc) from rank;[26Gnrank() over(order by col2 desc) from rank;[27Gsrank() over(order by col2 desc) from rank;[28Gerank() over(order by col2 desc) from rank;[29G_rank() over(order by col2 desc) from rank;[30G[72G
Query ID = cloudera_20200317090101_4162e0ac-2ef3-4ccf-8250-12925d064210
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1584300088534_0009, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1584300088534_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1584300088534_0009
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-03-17 09:01:57,426 Stage-1 map = 0%,  reduce = 0%
2020-03-17 09:02:19,972 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.49 sec
2020-03-17 09:02:45,710 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 3.49 sec
2020-03-17 09:02:49,249 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.82 sec
MapReduce Total cumulative CPU time: 4 seconds 820 msec
Ended Job = job_1584300088534_0009
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.82 sec   HDFS Read: 7842 HDFS Write: 186 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 820 msec
OK
col1	col2	_wcol0
leo	1500	1
Albert	1500	1
Lesa	1500	1
John	1500	1
John	1300	2
Lui	1300	2
Frank	1150	3
Loopa	1100	4
Mark	1000	5
John	900	6
Lesa	900	6
Pars	800	7
Bhut	800	7
leo	700	8
lock	650	9
Lio	500	10
Time taken: 82.94 seconds, Fetched: 16 row(s)
hive> select col1,col2,dense_rank() over(order by col2 desc) from rank;[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42G[41G[40G[39G[38G[37G[36G[35G[34G[33G() over(order by col2 desc) from rank;[K[33G[32G() over(order by col2 desc) from rank;[K[32G[31G() over(order by col2 desc) from rank;[K[31G[30G() over(order by col2 desc) from rank;[K[30G[29G() over(order by col2 desc) from rank;[K[29G[28G() over(order by col2 desc) from rank;[K[28G[27G() over(order by col2 desc) from rank;[K[27G[26G() over(order by col2 desc) from rank;[K[26G[25G() over(order by col2 desc) from rank;[K[25G[24G() over(order by col2 desc) from rank;[K[24G[23G() over(order by col2 desc) from rank;[K[23G,() over(order by col2 desc) from rank;[24Gr() over(order by col2 desc) from rank;[25Go() over(order by col2 desc) from rank;[26Gw() over(order by col2 desc) from rank;[27G_() over(order by col2 desc) from rank;[28Gn() over(order by col2 desc) from rank;[29Gu() over(order by col2 desc) from rank;[30Gm() over(order by col2 desc) from rank;[31Gb() over(order by col2 desc) from rank;[32Ge() over(order by col2 desc) from rank;[33Gr() over(order by col2 desc) from rank;[34G[72G
Query ID = cloudera_20200317090303_b36fe681-5cca-42ff-af64-46c8105da09e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1584300088534_0010, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1584300088534_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1584300088534_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-03-17 09:04:19,217 Stage-1 map = 0%,  reduce = 0%
2020-03-17 09:04:42,503 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.64 sec
2020-03-17 09:05:09,426 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 3.61 sec
2020-03-17 09:05:12,886 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.77 sec
MapReduce Total cumulative CPU time: 4 seconds 770 msec
Ended Job = job_1584300088534_0010
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.77 sec   HDFS Read: 7585 HDFS Write: 192 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 770 msec
OK
col1	col2	_wcol0
leo	1500	1
Albert	1500	2
Lesa	1500	3
John	1500	4
John	1300	5
Lui	1300	6
Frank	1150	7
Loopa	1100	8
Mark	1000	9
John	900	10
Lesa	900	11
Pars	800	12
Bhut	800	13
leo	700	14
lock	650	15
Lio	500	16
Time taken: 84.187 seconds, Fetched: 16 row(s)
hive> select col1,col2,row_number() over(order by col2 desc) from rank;[71G[70G[69G[68G[67G[66G[65G[64G[63G[62G[61G[60G[59G[58G[57G[56G[55G[54G[53G[52G[51G[50G[49G[48G[47G[46G[45G[44G[43G[42Gporder by col2 desc) from rank;[43Gaorder by col2 desc) from rank;[44Grorder by col2 desc) from rank;[45Gtorder by col2 desc) from rank;[46Giorder by col2 desc) from rank;[47Gtorder by col2 desc) from rank;[48Giorder by col2 desc) from rank;[49Goorder by col2 desc) from rank;[50Gnorder by col2 desc) from rank;[51G order by col2 desc) from rank;[52Gborder by col2 desc) from rank;[53Gyorder by col2 desc) from rank;[54G order by col2 desc) from rank;[55Gcorder by col2 desc) from rank;[56Goorder by col2 desc) from rank;[57Glorder by col2 desc) from rank;[58Gworder by col2 desc) from rank;[59G[58Gorder by col2 desc) from rank;[K[58G1order by col2 desc) from rank;[59G order by col2 desc) from rank;[60G[90G
Query ID = cloudera_20200317090505_4772062e-1c6b-4618-a0e6-6f4aebbd8e96
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1584300088534_0011, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1584300088534_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1584300088534_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-03-17 09:06:06,646 Stage-1 map = 0%,  reduce = 0%
2020-03-17 09:06:30,215 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.59 sec
2020-03-17 09:06:57,674 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 3.51 sec
2020-03-17 09:07:01,451 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.78 sec
MapReduce Total cumulative CPU time: 4 seconds 780 msec
Ended Job = job_1584300088534_0011
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.78 sec   HDFS Read: 7444 HDFS Write: 185 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 780 msec
OK
col1	col2	_wcol0
Albert	1500	1
Bhut	800	1
Frank	1150	1
John	1500	1
John	1300	2
John	900	3
Lesa	1500	1
Lesa	900	2
Lio	500	1
Loopa	1100	1
Lui	1300	1
Mark	1000	1
Pars	800	1
leo	1500	1
leo	700	2
lock	650	1
Time taken: 85.414 seconds, Fetched: 16 row(s)
hive> select * form rank;
FAILED: ParseException line 1:9 missing EOF at 'form' near '*'
hive> s[7G[Kselect * from jr[22G[K[21G[Krankl'[26G[K[25G[K;
OK
rank.col1	rank.col2
John	1500
Albert	1500
Mark	1000
Frank	1150
Loopa	1100
Lui	1300
John	1300
John	900
Lesa	1500
Lesa	900
Pars	800
leo	700
leo	1500
lock	650
Bhut	800
Lio	500
Time taken: 0.129 seconds, Fetched: 16 row(s)
hive> ]0;cloudera@quickstart:~[cloudera@quickstart ~]$ exit
exit

Script done on Tue 17 Mar 2020 09:10:32 AM PDT
